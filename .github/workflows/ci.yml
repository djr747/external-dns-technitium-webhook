name: CI

on:
  push:
    branches:
      - '**'
      - '!main'
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:

permissions:
  contents: read
  security-events: write
  packages: write
  id-token: write

jobs:
  lint:
    name: Lint and Format Check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run Ruff format check
        run: ruff format --check .

      - name: Run Ruff lint
        run: ruff check .

      - name: Run mypy
        run: mypy external_dns_technitium_webhook

      - name: Run pyright
        run: pyright

  test:
    name: Test Python ${{ matrix.python-version }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.13']

    steps:
      - uses: actions/checkout@v5

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run tests with coverage
        run: |
          pytest tests/unit/ --cov=external_dns_technitium_webhook --cov-report=xml --cov-report=html --cov-report=term-missing --cov-fail-under=95

      - name: Upload coverage report
        uses: actions/upload-artifact@v5
        if: matrix.python-version == '3.13'
        with:
          name: coverage-report
          path: htmlcov/
          retention-days: 30

      - name: Upload coverage to Codecov
        if: matrix.python-version == '3.13'
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false

  integration:
    name: Integration Tests (Kubernetes)
    runs-on: ubuntu-latest
    needs: [test, docker-build]
    steps:
      - uses: actions/checkout@v5
        with:
          fetch-depth: 0

      - name: Create Kind cluster config
        run: |
          cat > /tmp/kind-config.yaml <<EOF
          kind: Cluster
          apiVersion: kind.x-k8s.io/v1alpha4
          nodes:
            - role: control-plane
              extraPortMappings:
                - containerPort: 5380
                  hostPort: 5380
                  protocol: TCP
                - containerPort: 53
                  hostPort: 5353
                  protocol: UDP
          EOF

      - name: Set up Kind cluster
        uses: helm/kind-action@v1.10.0
        with:
          cluster_name: integration-test
          config: /tmp/kind-config.yaml
          version: v0.23.0
          kubectl_version: v1.31.0

      - name: Build and load webhook image into Kind
        run: |
          docker build -t external-dns-technitium-webhook:latest .
          kind load docker-image external-dns-technitium-webhook:latest --name integration-test

      - name: Pull and load Technitium image into Kind
        run: |
          docker pull technitium/dns-server:latest
          kind load docker-image technitium/dns-server:latest --name integration-test

      - name: Generate test credentials
        id: creds
        run: |
          PASSWORD=$(openssl rand -base64 12)
          echo "admin_password=$PASSWORD" >> $GITHUB_OUTPUT
          echo "Generated admin password: $PASSWORD"

      - name: Create Technitium secret
        run: |
          kubectl create secret generic technitium-secret \
            --from-literal=username=admin \
            --from-literal=password=${{ steps.creds.outputs.admin_password }} \
            -n default

      - name: Deploy Technitium DNS
        run: |
          kubectl apply -f tests/integration/k8s/technitium-deployment.yaml

      - name: Wait for Technitium deployment
        run: |
          kubectl wait --for=condition=ready pod \
            -l app=technitium \
            --timeout=300s \
            -n default

      - name: Run Technitium initialization script
        run: |
          kubectl run technitium-init \
            --image=curlimages/curl:latest \
            --restart=Never \
            --attach=true \
            --quiet \
            --env="TECHNITIUM_URL=http://technitium:5380" \
            --env="ADMIN_PASSWORD=${{ steps.creds.outputs.admin_password }}" \
            --env="CATALOG_ZONE=catalog.invalid" \
            -- sh -c "$(cat tests/integration/fixtures/init-technitium.sh)"
          
          INIT_EXIT_CODE=$?
          
          # Get logs before cleanup
          echo ""
          echo "=== Technitium Init Pod Logs ==="
          kubectl logs technitium-init -n default || true
          
          # Clean up the pod - force delete if needed
          kubectl delete pod technitium-init -n default --force --grace-period=0 2>/dev/null || true
          
          # Fail if initialization failed
          if [ $INIT_EXIT_CODE -ne 0 ]; then
            echo ""
            echo "ERROR: Technitium initialization failed with exit code $INIT_EXIT_CODE"
            exit 1
          fi

      - name: Add External DNS Helm repository
        run: |
          helm repo add external-dns https://kubernetes-sigs.github.io/external-dns/
          helm repo update

      - name: Deploy External DNS with webhook
        run: |
          helm install external-dns external-dns/external-dns \
            -f tests/integration/helm/external-dns-values.yaml \
            -n default \
            --timeout=5m

      - name: Wait for External DNS deployment
        run: |
          kubectl wait --for=condition=ready pod \
            -l app.kubernetes.io/name=external-dns \
            --timeout=300s \
            -n default || true

      - name: Check External DNS pod status
        run: |
          kubectl get pods -n default
          kubectl describe pod -l app.kubernetes.io/name=external-dns -n default || true
          kubectl logs -l app.kubernetes.io/name=external-dns -n default --tail=50 || true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'
          cache: 'pip'

      - name: Install integration test dependencies
        run: |
          pip install pytest pytest-asyncio httpx kubernetes

      - name: Run integration tests
        env:
          TECHNITIUM_URL: http://localhost:5380
          WEBHOOK_URL: http://localhost:8888
        run: |
          pytest tests/integration/test_webhook_integration.py \
            -v \
            --tb=short \
            -m "" || true

      - name: Collect logs on failure
        if: failure()
        run: |
          echo "=== Kubernetes Pods ==="
          kubectl get pods -n default
          
          echo ""
          echo "=== Technitium Logs ==="
          kubectl logs -l app=technitium -n default --tail=100 || true
          
          echo ""
          echo "=== External DNS Logs ==="
          kubectl logs -l app.kubernetes.io/name=external-dns -n default --tail=100 || true
          
          echo ""
          echo "=== Webhook Sidecar Logs ==="
          kubectl logs -l app.kubernetes.io/name=external-dns -n default -c webhook-provider --tail=100 || true

      - name: Cleanup Kind cluster
        if: always()
        run: |
          kind delete cluster --name integration-test

  docker-build:
    name: Build Docker Image and Scan
    runs-on: ubuntu-latest
    needs: [test]
    permissions:
      contents: read
      packages: write
      security-events: write
      id-token: write
      attestations: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          fetch-depth: 0

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata (tags, labels)
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ghcr.io/${{ github.repository }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            # Use a stable prefix for SHA tags to avoid generating tags that
            # begin with a hyphen when the branch name is empty in certain
            # CI contexts (which produced invalid tags like ":-489026a").
            type=sha,prefix=commit-
            type=raw,value=ci-${{ github.sha }},enable={{is_default_branch}}

      - name: Build and push Docker image
        id: build
        uses: docker/build-push-action@v6
        with:
          context: .
          platforms: linux/amd64,linux/arm64
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          sbom: true
          provenance: true

      - name: Generate artifact attestation
        if: success()
        uses: actions/attest-build-provenance@v1
        with:
          subject-name: ghcr.io/${{ github.repository }}
          subject-digest: ${{ steps.build.outputs.digest }}
          push-to-registry: true

      - name: Verify built image Python version matches Chainguard latest
        run: |
          echo "Checking Python version inside built image..."
          chainguard_version=$(docker run --rm --entrypoint python cgr.dev/chainguard/python:latest -c "import sys; print(f'{sys.version_info.major}.{sys.version_info.minor}')")
          # Construct an explicit image reference using the pushed digest (repository@digest).
          # For multi-platform builds the 'image' output may be empty; the digest is a reliable identifier.
          IMAGE_REF="ghcr.io/${{ github.repository }}@${{ steps.build.outputs.digest }}"
          echo "Pulling built image by digest: $IMAGE_REF"
          docker pull "$IMAGE_REF"
          image_version=$(docker run --rm --entrypoint python "$IMAGE_REF" -c "import sys; print(f'{sys.version_info.major}.{sys.version_info.minor}')")
          if [ "$image_version" != "$chainguard_version" ]; then
            echo "ERROR: Python version in built image ($image_version) does not match Chainguard latest ($chainguard_version)"
            exit 1
          fi
          echo "Python version in built image matches Chainguard latest: $image_version"
